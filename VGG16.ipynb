{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOH02W1ohMObNzw86AEt9l9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mr-1504/Vigilant-VGG16/blob/dev/VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V12d5kISu5ht"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Train')\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from Dataset import Yaw\n",
        "from torchvision.transforms import Compose, ToTensor, Resize, RandomRotation, RandomHorizontalFlip, ColorJitter, GaussianBlur\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from tqdm.autonotebook import tqdm\n",
        "import warnings\n",
        "import torchvision.models as models\n",
        "import random\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import cv2\n",
        "\n",
        "def plot_confusion_matrix(writer, cm, class_names, epoch):\n",
        "    figure = plt.figure(figsize=(20, 20))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=\"plasma\")\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    threshold = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            color = \"white\" if cm[i, j] > threshold else \"black\"\n",
        "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    writer.add_figure('confusion_matrix', figure, epoch)\n",
        "\n",
        "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Stroke'])\n",
        "    cm_display.plot()\n",
        "    plt.show()\n",
        "\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    print(\"\\n\\nMetrics:\")\n",
        "    print(\"Accuracy: \", round(accuracy, 2))\n",
        "    print(\"Precision: \", round(precision, 2))\n",
        "    print(\"Recall: \", round(recall, 2))\n",
        "    print(\"F1-score: \", round(f1_score, 2))\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='Classifier')\n",
        "    parser.add_argument('-p', '--data_path', type=str, default=\"data\")\n",
        "    parser.add_argument('-b', '--batch_size', type=int, default=64)\n",
        "    parser.add_argument('-e', '--epochs', type=int, default=50)\n",
        "    parser.add_argument('-l', '--lr', type=float, default=1e-3)\n",
        "    parser.add_argument('-s', '--image_size', type=int, default=224)\n",
        "    parser.add_argument('-c', '--checkpoint_path', type=str, default=None)\n",
        "    parser.add_argument('-t', '--tensorboard_path', type=str, default=\"/content/drive/MyDrive/Train/tensorboard\")\n",
        "    parser.add_argument('-r', '--trained_path', type=str, default=\"/content/drive/MyDrive/Train/trained_models\")\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def train(args, train_accuracies_epochs, train_loss_value_epochs, val_accuracies_epochs, val_loss_value_epochs):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Resize((args.image_size, args.image_size)),\n",
        "        RandomRotation(degrees=15),\n",
        "        RandomHorizontalFlip(p=0.5),\n",
        "        ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),\n",
        "        GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0))\n",
        "    ])\n",
        "    train_set = Yaw(root=args.data_path, train=True, transform=transform)\n",
        "    valid_set = Yaw(root=args.data_path, train=False, transform=transform)\n",
        "\n",
        "    sample_images = []\n",
        "    for i in range(10):\n",
        "        random_integer = random.randint(0, len(train_set)-1)\n",
        "        image, _ = train_set[random_integer]\n",
        "        sample_images.append(image)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "    axes = axes.flatten()\n",
        "    for ax, img in zip(axes, sample_images):\n",
        "        arrImg = img.numpy().transpose(1, 2, 0)\n",
        "        cvImg = cv2.cvtColor(arrImg, cv2.COLOR_RGB2BGR)\n",
        "        ax.imshow(cvImg)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    training_params = {\n",
        "        \"batch_size\": args.batch_size,\n",
        "        \"shuffle\": True,\n",
        "        \"drop_last\": True,\n",
        "        \"num_workers\": 6\n",
        "    }\n",
        "\n",
        "    valid_params = {\n",
        "        \"batch_size\": args.batch_size,\n",
        "        \"shuffle\": False,\n",
        "        \"drop_last\": False,\n",
        "        \"num_workers\": 6\n",
        "    }\n",
        "    train_dataloader = DataLoader(train_set, **training_params)\n",
        "    valid_dataloader = DataLoader(valid_set, **valid_params)\n",
        "\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    new_layers = nn.Sequential(\n",
        "        nn.Linear(in_features=25088, out_features=4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(in_features=4096, out_features=4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(in_features=4096, out_features=2)\n",
        "    )\n",
        "\n",
        "    model.classifier = new_layers\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for param in model.features[-4:].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)\n",
        "\n",
        "    if args.checkpoint_path and os.path.isfile(args.checkpoint_path):\n",
        "        checkpoint = torch.load(args.checkpoint_path)\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "        start_epoch = checkpoint[\"epoch\"] + 1\n",
        "        best_acc = checkpoint[\"best_acc\"]\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "        best_acc = 0\n",
        "\n",
        "    if os.path.isdir(args.tensorboard_path):\n",
        "        shutil.rmtree(args.tensorboard_path)\n",
        "    os.mkdir(args.tensorboard_path)\n",
        "    if not os.path.isdir(args.trained_path):\n",
        "        os.mkdir(args.trained_path)\n",
        "    writer = SummaryWriter(args.tensorboard_path)\n",
        "    num_iters = len(train_dataloader)\n",
        "\n",
        "    conf_matrix = None\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        loss_value_iter = []\n",
        "        correct_sample = 0\n",
        "        total_sample = 0\n",
        "\n",
        "        progress_bar = tqdm(train_dataloader, colour=\"yellow\")\n",
        "        for iter, (images, labels) in enumerate(progress_bar):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            predictions = model(images)\n",
        "            loss = criterion(predictions, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_value = loss.item()\n",
        "\n",
        "            _, predicted = torch.max(predictions.data, 1)\n",
        "            total_sample += labels.size(0)\n",
        "            correct_sample += (predicted == labels).sum().item()\n",
        "\n",
        "            progress_bar.set_description(\"Epoch {}/{}. Loss value: {:.4f}\".format(epoch + 1, args.epochs, loss_value))\n",
        "            losses.append(loss_value)\n",
        "            writer.add_scalar(\"Train/Loss\", np.mean(losses), epoch*num_iters+iter)\n",
        "            loss_value_iter.append(loss_value)\n",
        "\n",
        "        epoch_accuracy = correct_sample / total_sample\n",
        "        train_accuracies_epochs.append(epoch_accuracy)\n",
        "        train_loss_value_epochs.append(np.mean(loss_value_iter))\n",
        "\n",
        "        model.eval()\n",
        "        losses = []\n",
        "        all_predictions = []\n",
        "        all_gts = []\n",
        "        val_loss_value_iter = []\n",
        "        with torch.no_grad():\n",
        "            for iter, (images, labels) in enumerate(valid_dataloader):\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                predictions = model(images)\n",
        "                max_idx = torch.argmax(predictions, 1)\n",
        "                loss = criterion(predictions, labels)\n",
        "                losses.append(loss.item())\n",
        "                all_gts.extend(labels.tolist())\n",
        "                all_predictions.extend(max_idx.tolist())\n",
        "                val_loss_value_iter.append(loss.item())\n",
        "        val_loss_value_epochs.append(np.mean(val_loss_value_iter))\n",
        "        writer.add_scalar(\"Val/Loss\", np.mean(losses), epoch)\n",
        "        acc = accuracy_score(all_gts, all_predictions)\n",
        "        val_accuracies_epochs.append(acc)\n",
        "        writer.add_scalar(\"Val/Accuracy\", acc, epoch)\n",
        "        conf_matrix = confusion_matrix(all_gts, all_predictions)\n",
        "\n",
        "        checkpoint = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"epoch\": epoch,\n",
        "            \"best_acc\": best_acc,\n",
        "            \"batch_size\": args.batch_size\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, os.path.join(args.trained_path, \"last.pt\"))\n",
        "        if acc > best_acc:\n",
        "            torch.save(checkpoint, os.path.join(args.trained_path, \"best.pt\"))\n",
        "            best_acc = acc\n",
        "\n",
        "        scheduler.step()\n",
        "    plot_confusion_matrix(writer, conf_matrix, [i for i in range(len(train_set.categories))], epoch)\n",
        "\n",
        "def plot_phase(train_accuracies_epochs, train_loss_value_epochs, val_accuracies_epochs, val_loss_value_epochs):\n",
        "    epochs = range(1, len(train_accuracies_epochs) + 1)\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_accuracies_epochs, color='blue', label='Train')\n",
        "    plt.plot(epochs, val_accuracies_epochs, color='orange', linestyle='--', label='Test')\n",
        "    plt.title('Training and Test Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_loss_value_epochs, color='blue', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss_value_epochs, color='orange', linestyle='--', label='Test Loss')\n",
        "    plt.title('Training and Test Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/Train/VGG_ne.png')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_args()\n",
        "    train_accuracies_epochs = []\n",
        "    train_loss_value_epochs = []\n",
        "    val_accuracies_epochs = []\n",
        "    val_loss_value_epochs = []\n",
        "    train(args, train_accuracies_epochs, train_loss_value_epochs, val_accuracies_epochs, val_loss_value_epochs)\n",
        "    plot_phase(train_accuracies_epochs, train_loss_value_epochs, val_accuracies_epochs, val_loss_value_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "EF_FtJpowL8z",
        "outputId": "55876e2c-fecf-4666-a213-62cdd68acb97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    }
  ]
}